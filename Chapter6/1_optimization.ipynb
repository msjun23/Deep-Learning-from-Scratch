{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_optimization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPHby40pxV4rRV1rGZk8f8M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msjun23/Deep-Learning-from-Scratch/blob/main/Chapter6/1_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSf3vdsENx9c"
      },
      "source": [
        "# 최적화\r\n",
        "\r\n",
        "손실 함수의 값을 가능한 낮추는 것."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n26lu608QxsG"
      },
      "source": [
        "**SGD**, 확률적 경사 하강법의 수식은 다음과 같이 쓸 수 있다.\r\n",
        "$$\\textbf W\\leftarrow\\textbf W-\\eta\\frac{\\partial L}{\\partial \\textbf W}$$\r\n",
        "\r\n",
        "여기서 $\\eta$는 학습률(learning rate)이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nvWKn1lNk2v"
      },
      "source": [
        "# 확률적 경사 하강법(SGD)\r\n",
        "class SGD:\r\n",
        "  def __init__(self, lr=0.01):\r\n",
        "    self.lr = lr\r\n",
        "\r\n",
        "  def update(sefl, params, grads):\r\n",
        "    for key in params.keys():\r\n",
        "      params[key] -= self.lr * grads[key]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mATp6ffqPALk"
      },
      "source": [
        "```\r\n",
        "# pseudo code 예시\r\n",
        "network = TwoLayerNet(...)\r\n",
        "optimizer = SGD()\r\n",
        "\r\n",
        "for i in range(10000):\r\n",
        "  ...\r\n",
        "  x_batch, t_batch = get_mini_batch(...)\r\n",
        "  grads = network.gradient(x_batch, t_batch)\r\n",
        "  params = network.params\r\n",
        "  optimizer.update(params, grads)\r\n",
        "  ...\r\n",
        "```\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBLtDXJkP-n_"
      },
      "source": [
        "SGD의 단점\r\n",
        "\r\n",
        "- 한 방향으로의 기울기가 가파르고 다른 방향으로의 기울기가 완만하다면, 기울기가 가파른 방향으로 요동치며 최적화가 진행된다. 이는 상당히 비효율적인 움직임이다.\r\n",
        "- 비등방성(anisotropy) 함수에서는 탐색 경로가 비효율적이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfmzzK9wQoFR"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5AvQNQRRk8l"
      },
      "source": [
        "**모멘텀**(Momentum)은 속도(가속)의 정보를 수식에 추가했다고 보면 된다.\r\n",
        "$$\\textbf v\\leftarrow\\alpha\\textbf v-\\eta\\frac{\\partial L}{\\partial \\textbf W}\r\n",
        "\\\\\r\n",
        "\\textbf W\\leftarrow\\textbf W+\\textbf v$$\r\n",
        "\r\n",
        "공이 곡면(기울기)을 따라 움직이는 모습을 상상하면 된다. 움직일 수록 가속도를 받아 더욱 빠르게 경사를 따라 하강할 것이다. 위의 식에서 $\\alpha\\textbf v$항은 물체가 아무런 힘을 받지 않을 때 서서히 하강시키는 역할을 한다. $\\alpha$는 보통 0.9 등으로 설정한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwzEWM4eTM9N"
      },
      "source": [
        "# Momentum 구현\r\n",
        "class Momentum:\r\n",
        "  def __init__(self, lr=0.01, momentum=0.9):\r\n",
        "    sefl.lr = lr\r\n",
        "    self.momentum = momentum\r\n",
        "    sefl.v = None\r\n",
        "\r\n",
        "  def update(sefl, params, grads):\r\n",
        "    if self.v is None:\r\n",
        "      sefl.v = {}\r\n",
        "      for key, val in params.items():\r\n",
        "        self.v[key] = np.zeros_like(val)\r\n",
        "\r\n",
        "    for key in params.keys():\r\n",
        "      self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\r\n",
        "      params[key] += self.v[key]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR0wjn8CUf0x"
      },
      "source": [
        "SGD와 비교했을 때 덜 지그재그로 움직인다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB9TMwXwVkCl"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHsM8S9HVnEn"
      },
      "source": [
        "**AdaGrad**는 각각의 매개변수에 적응적으로 학습률을 조정하면서 학습을 진행한다. AdaGrad의 수식은 다음과 같다.\r\n",
        "$$\\textbf h\\leftarrow\\textbf h+\\frac{\\partial L}{\\partial\\textbf W}\\bigodot\\frac{\\partial L}{\\partial\\textbf W}\r\n",
        "\\\\\r\n",
        "\\textbf W\\leftarrow\\textbf W-\\eta\\frac{1}{\\sqrt{\\textbf h}}\\frac{\\partial L}{\\partial\\textbf W}$$\r\n",
        "\r\n",
        "AdaGrad는 기존 기울기의 값을 제곱하여 계속 더해준다($\\textbf h$).그리고 매개변수를 갱신할 때 $\\textbf h$의 제곱근으로 learning rate를 나눠 학습률을 조정한다.\r\n",
        "\r\n",
        "매개변수의 원소 중에서 많이 움직인(크게 갱신되) 원소는 학습률이 낮아진다는 것이데, 다시 말해 학습률 감소가 매개변수의 원소마다 다르게 적용된다는 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M_v2g-SUeF1"
      },
      "source": [
        "# AdaGrad 구현\r\n",
        "class AdaGrad:\r\n",
        "  def __init__(self, lr=0.01):\r\n",
        "    self.lr = lr\r\n",
        "    self.h = None\r\n",
        "\r\n",
        "  def update(self, params, grads):\r\n",
        "    if self.h is None:\r\n",
        "      self.h = {}\r\n",
        "      for key, val in params.items():\r\n",
        "        self.h[key] = np.zeros_like(val)\r\n",
        "      \r\n",
        "    for key in params.keys():\r\n",
        "      self.h[key] += grads[key] * grads[key]\r\n",
        "      params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M10rB4guYku8"
      },
      "source": [
        "최솟값을 향해 효율적으로 움직인다. 예를 들어 y축 방향으로 기울기가 크다면, 처음에는 y축으로 크게 움직이지만, 그 큰 움직임에 비례해 갱신 정도도 큰 폭으로 작아지도록 조정된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vio9lONeY7sN"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzBD2oxnY95w"
      },
      "source": [
        "모멘텀과 AdaGrad를 융합한 것이 **Adam**이다. 모멘텀과 비슷한 최적화 패턴을 보이나 모멘텀보다 반동이 적다.이는 학습의 갱신 정도를 적응적으로 조정해서 얻는 효과이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLITEaaeYX6t"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    }
  ]
}