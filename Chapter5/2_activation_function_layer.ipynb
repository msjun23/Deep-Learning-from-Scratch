{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_activation_function_layer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMwyZtdkCaSG9KwJSO4Vm2n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msjun23/Deep-Learning-from-Scratch/blob/main/Chapter5/2_activation_function_layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uqd6MTpVozU"
      },
      "source": [
        "신경망을 구성하는 계층 각각을 하나의 클래스로 구현한다.\r\n",
        "\r\n",
        "# ReLU layer\r\n",
        "활성화 함수로 사용된는 **ReLU** 계층의 수식은 다음과 같다.\r\n",
        "$$y=\\left\\{\\begin{matrix}\r\n",
        "x\\;\\;(x>0)\\\\ \r\n",
        "0\\;\\;(x\\leq 0)\r\n",
        "\\end{matrix}\\right.$$\r\n",
        "\r\n",
        "위의 식에서 $x$에 대한 $y$의 미분은 아래와 같다.\r\n",
        "$$\\frac{\\partial y}{\\partial x}=\\left\\{\\begin{matrix}\r\n",
        "1\\;\\;(x>0)\\\\ \r\n",
        "0\\;\\;(x\\leq 0)\r\n",
        "\\end{matrix}\\right.$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2hVlYLbUIUn"
      },
      "source": [
        "# ReLU 함수를 가진 계층 구현\r\n",
        "class ReLU:\r\n",
        "  def __init__(self):\r\n",
        "    self.mask = None\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    self.mask = (x <= 0)\r\n",
        "    out = x.copy()\r\n",
        "    out[self.mask] = 0\r\n",
        "\r\n",
        "    return 0\r\n",
        "\r\n",
        "  def backward(self, dout):\r\n",
        "    dout[self.mask] = 0\r\n",
        "    dx = dout\r\n",
        "\r\n",
        "    return dx"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW2nkQr7X9oz",
        "outputId": "0d90b49f-087b-4eb6-d81c-f87804d750cf"
      },
      "source": [
        " import numpy as np\r\n",
        "\r\n",
        " x = np.array([[1.0, -0.5], [-2.0, 3.0]])\r\n",
        " print(x)\r\n",
        "\r\n",
        " mask = (x <= 0)\r\n",
        " print(mask)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.  -0.5]\n",
            " [-2.   3. ]]\n",
            "[[False  True]\n",
            " [ True False]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42XJ3Go5YWaH"
      },
      "source": [
        "# Sigmoid layer\r\n",
        "\r\n",
        "Sigmoid 함수는 다음과 같다.\r\n",
        "$$y=\\frac{1}{1+\\text{exp}(-x)}$$\r\n",
        "\r\n",
        "Simgoid 함수를 $x$에 대해 미분하면 아래와 같다.\r\n",
        "$$\\frac{\\partial y}{\\partial x}=\\frac{e^{-x}}{(1+e^{-x})^2}\r\n",
        "\\\\=\\frac{1+e^{-x}-1}{(1+e^{-x})^2}\r\n",
        "\\\\=y(1-y)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaFSAkYdYOoD"
      },
      "source": [
        "# Sigmoid 함수를 가진 계층 구현\r\n",
        "class Sigmoid():\r\n",
        "  def __init__(self):\r\n",
        "    self.out = None\r\n",
        "  \r\n",
        "  def forward(self, x):\r\n",
        "    out = 1 / (1 + np.exp(-x))\r\n",
        "    self.out = out\r\n",
        "\r\n",
        "    return out\r\n",
        "\r\n",
        "  def backward(self, dout):\r\n",
        "    dx = dout * (1.0 - self.out) * self.out\r\n",
        "\r\n",
        "    return dx"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV3clzKnsVZU"
      },
      "source": [
        "# Affine layer\r\n",
        "\r\n",
        "신경망에서 순전파 때 수행하는 행렬의 곱은 **어파인 변환**(affine transformation)이라고 한다. 이러한 어파인 변환을 수행하는 처리를 Affine 계층으로 구현한다.\r\n",
        "\r\n",
        "행렬 데이터에 대해 역전파를 전개하면 아래와 같다.\r\n",
        "$$\\frac{\\partial L}{\\partial X}=\\frac{\\partial L}{\\partial Y}\\cdot W^T\r\n",
        "\\\\ \\frac{\\partial L}{\\partial W}=X^T\\cdot\\frac{\\partial L}{\\partial Y}$$\r\n",
        "\r\n",
        "$W^T$는 **전치행렬**을 의미한다. 전치행렬이란 행렬 $(i, j)$ 위치의 원소를 $(j, i)$ 위치로 이동하는 것을 말한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdmNcZ65qKOa",
        "outputId": "bdf12ecf-e6bd-4753-9940-b218d48392c2"
      },
      "source": [
        "# 데이터 N개를 묶어 순전파 진행을 하는 경우\r\n",
        "# 배치용 Affine 계층\r\n",
        "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\r\n",
        "B = np.array([1, 2, 3])\r\n",
        "\r\n",
        "print(X_dot_W)\r\n",
        "\r\n",
        "print(X_dot_W + B)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0]\n",
            " [10 10 10]]\n",
            "[[ 1  2  3]\n",
            " [11 12 13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QF2L4JQswtnJ",
        "outputId": "c200ab16-2d87-4c8c-dd29-889b491d9efe"
      },
      "source": [
        "# 역전파\r\n",
        "dY = np.array([[1, 2, 3], [4, 5, 6]])\r\n",
        "print(dY)\r\n",
        "\r\n",
        "dB = np.sum(dY, axis=0)\r\n",
        "print(dB)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2 3]\n",
            " [4 5 6]]\n",
            "[5 7 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJbjp0vdxjx5"
      },
      "source": [
        "# Affine 구현\r\n",
        "class Affine:\r\n",
        "  def __init__(self, W, b):\r\n",
        "    self.W = W\r\n",
        "    self.b = b\r\n",
        "    self.x = None\r\n",
        "    self.dW = None\r\n",
        "    self.db = None\r\n",
        "    \r\n",
        "  def forward(self, x):\r\n",
        "    self.x = x\r\n",
        "    out = np.dot(x, self.W) + self.b\r\n",
        "\r\n",
        "    return out\r\n",
        "\r\n",
        "  def backward(self, dout):\r\n",
        "    dx = np.dot(dout, self.W.T)\r\n",
        "    self.dW = np.dot(self.x.T, dout)\r\n",
        "    self.db = np.sum(dout, axis=0)\r\n",
        "\r\n",
        "    return dx"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqTBOfOz5x3u"
      },
      "source": [
        "# Softmax-with-Loss layer\r\n",
        "\r\n",
        "소프트맥스 함수는 주로 출력층에서 사용한다. 소프트맥스는 입력값을 정규화하여 출력한다. 소프트맥스는 문제를 확률적으로 접근할 수 있게 해준다.\r\n",
        "\r\n",
        "소프트맥스 함수와 교차 엔트로피 오차 함수를 같이 사용한다. 그 결과 소프트맥스 계층의 역전파는(y - t)로 깔끔하게 나온다. 이는 교차 엔트로피 오차 함수가 애초에 그렇게 설계되었기 때문이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xogvjQIzzIhv"
      },
      "source": [
        "# Softmax-with-loss 계층 구현\r\n",
        "class SoftmaxWithLoss:\r\n",
        "  def __init__(self):\r\n",
        "    self.loss = None    # 손실\r\n",
        "    self.y = None       # softmax의 출력\r\n",
        "    self.t = None       # 정답 레이블(원-핫 벡터)\r\n",
        "\r\n",
        "  def forward(self, x, t):\r\n",
        "    self.t = t\r\n",
        "    self.y = softmax(x)\r\n",
        "    self.loss = cross_entropy_error(self.y, self.t)\r\n",
        "\r\n",
        "    return self.loss\r\n",
        "\r\n",
        "  def backward(self, dout=1):\r\n",
        "    batch_size = self.t.shape[0]\r\n",
        "    dx = (self.y - self.t) / batch_size"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2g5VFBy9_Vy"
      },
      "source": [
        "# 복습 : softmax, cross_entropy_error 구현\r\n",
        "def softmax(a):\r\n",
        "  c = np.max(a)\r\n",
        "  exp_a = np.exp(a - c)   # 오버플로 대책\r\n",
        "  sum_exp_a = np.sum(exp_a)\r\n",
        "  y = exp_a / sum_exp_a\r\n",
        "\r\n",
        "  return y\r\n",
        "\r\n",
        "def cross_entropy_error(y, t):\r\n",
        "  if y.ndim == 1:\r\n",
        "    t = t.reshape(1, t.size)\r\n",
        "    y = y.reshape(1, y.size)\r\n",
        "\r\n",
        "  batch_size = y.shape[0]\r\n",
        "  return -np.sum(t * np.log(y + 1e-7)) / batch_size"
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}