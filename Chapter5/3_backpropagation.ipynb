{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_backpropagation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPdtXrjtAuY92Yt0i4zTQ76",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msjun23/Deep-Learning-from-Scratch/blob/main/Chapter5/3_backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ulRQDdR_f4H"
      },
      "source": [
        "신경망 학습이란\r\n",
        "\r\n",
        "0. 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 **'학습'**이라 한다.\r\n",
        "\r\n",
        "1. 훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표이다.\r\n",
        "\r\n",
        "2. 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게하는 방향을 제시한다.\r\n",
        "\r\n",
        "3. 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\r\n",
        "\r\n",
        "4. 반복"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU_sKnLs_WVB"
      },
      "source": [
        "# 오차역전법(backpropagation)을 적용한 신경망 구현 예제\r\n",
        "import sys, os\r\n",
        "sys.path.append(os.pardir)\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from common.layers import *\r\n",
        "from common.gradient import numerical_gradient\r\n",
        "from collections import OrderedDict\r\n",
        "\r\n",
        "class TwoLayerNet:\r\n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\r\n",
        "    # 가중치 초기화\r\n",
        "    self.params = {}\r\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\r\n",
        "    self.params['b1'] = np.zeros(hidden_size)\r\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\r\n",
        "    self.params['b2'] = np.zeros(output_size)\r\n",
        "\r\n",
        "    # 계층 생성\r\n",
        "    self.layers = OrderedDict()\r\n",
        "    self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\r\n",
        "    self.layers['ReLU1'] = Relu()\r\n",
        "    self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\r\n",
        "\r\n",
        "    self.lastLayer = SoftmaxWithLoss()\r\n",
        "\r\n",
        "  def predict(self, x):\r\n",
        "    for layer in self.layers.values():\r\n",
        "      x = layer.forward(x)\r\n",
        "\r\n",
        "    return x\r\n",
        "\r\n",
        "  def loss(self, x, t):\r\n",
        "    y = self.predict(x)\r\n",
        "    return self.lastLayer.forward(y, t)\r\n",
        "\r\n",
        "  def accuracy(self, x, t):\r\n",
        "    y = self.predict(x)\r\n",
        "    y = np.argmax(y, axis=1)\r\n",
        "    if t.ndim != 1:\r\n",
        "      t = np.argmax(t, axis=1)\r\n",
        "\r\n",
        "    accuracy = np.sum(y == t) / float(x.shape[0])\r\n",
        "    return accuracy\r\n",
        "\r\n",
        "  def numerical_gradient(self, x, t):\r\n",
        "    loss_W = lambda W: self.loss(x, t)\r\n",
        "\r\n",
        "    grads = {}\r\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\r\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\r\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\r\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\r\n",
        "    return grads\r\n",
        "\r\n",
        "  def gradient(self, x, t):\r\n",
        "    # 순전파\r\n",
        "    self.loss(x, t)\r\n",
        "\r\n",
        "    # 역전파\r\n",
        "    dout = 1\r\n",
        "    dout = self.lastLayer.backward(dout)\r\n",
        "\r\n",
        "    layers = list(self.layers.values())\r\n",
        "    layers.reverse()\r\n",
        "    for layer in layers:\r\n",
        "      dout = layer.backward(dout)\r\n",
        "\r\n",
        "    # 결과 저장\r\n",
        "    grads = {}\r\n",
        "    grads['W1'] = self.layers['Affine1'].dW\r\n",
        "    grads['b1'] = self.layers['Affine1'].db\r\n",
        "    grads['W2'] = self.layers['Affine2'].dW\r\n",
        "    grads['b2'] = self.layers['Affine2'].db\r\n",
        "\r\n",
        "    return grads"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttqd9Q8VA3uu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a833d659-4be0-4a63-da43-271001634408"
      },
      "source": [
        "# 기울기 확인(gradient check) 구현\r\n",
        "from mnist import load_mnist\r\n",
        "\r\n",
        "# 데이터 읽기\r\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\r\n",
        "\r\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\r\n",
        "\r\n",
        "x_batch = x_train[:3]\r\n",
        "t_batch = t_train[:3]\r\n",
        "\r\n",
        "grad_numerical = network.numerical_gradient(x_batch, t_batch)\r\n",
        "grad_backprop = network.gradient(x_batch, t_batch)\r\n",
        "\r\n",
        "# 각 가중치 차이의 절댓값을 구한 후, 그 절댓값들의 평균을 구한다.\r\n",
        "for key in grad_numerical.keys():\r\n",
        "  diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\r\n",
        "  print(key + ' : ' + str(diff))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 : 3.528001631324997e-10\n",
            "b1 : 2.384130566039015e-09\n",
            "W2 : 4.402300245891655e-09\n",
            "b2 : 1.3970391052609e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZxSspcgQb4K",
        "outputId": "eb177e38-72ed-4daf-9291-61d200b25c2e"
      },
      "source": [
        "# 오차역전법(backpropagation)을 사용한 학습 구현\r\n",
        "\r\n",
        "# 데이터 읽기\r\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\r\n",
        "network = TwoLayerNet(input_size=28*28, hidden_size=50, output_size=10)\r\n",
        "\r\n",
        "iters_num = 10000\r\n",
        "train_size = x_train.shape[0]\r\n",
        "batch_size = 100\r\n",
        "learning_rate = 0.1\r\n",
        "\r\n",
        "train_loss_list = []\r\n",
        "train_acc_list = []\r\n",
        "test_acc_list = []\r\n",
        "\r\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\r\n",
        "\r\n",
        "for i in range(iters_num):\r\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\r\n",
        "  x_batch = x_train[batch_mask]\r\n",
        "  t_batch = t_train[batch_mask]\r\n",
        "\r\n",
        "  # 오차역전법으로 기울기를 구한다.\r\n",
        "  grad = network.gradient(x_batch, t_batch)\r\n",
        "\r\n",
        "  # 갱신\r\n",
        "  for key in ('W1', 'b1', 'W2', 'b2'):\r\n",
        "    network.params[key] -= learning_rate * grad[key]\r\n",
        "\r\n",
        "  loss = network.loss(x_batch, t_batch)\r\n",
        "  train_loss_list.append(loss)\r\n",
        "\r\n",
        "  if i % iter_per_epoch == 0:\r\n",
        "    train_acc = network.accuracy(x_train, t_train)\r\n",
        "    test_acc = network.accuracy(x_test, t_test)\r\n",
        "    train_acc_list.append(train_acc)\r\n",
        "    test_acc_list.append(test_acc)\r\n",
        "    print('step ' + str(i) + ' : ', train_acc, test_acc)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0 :  0.11001666666666667 0.1186\n",
            "step 600 :  0.9013833333333333 0.9036\n",
            "step 1200 :  0.9229833333333334 0.9247\n",
            "step 1800 :  0.93495 0.9357\n",
            "step 2400 :  0.9459166666666666 0.9436\n",
            "step 3000 :  0.9492333333333334 0.9471\n",
            "step 3600 :  0.9573666666666667 0.9539\n",
            "step 4200 :  0.961 0.958\n",
            "step 4800 :  0.9630833333333333 0.9583\n",
            "step 5400 :  0.9674833333333334 0.9621\n",
            "step 6000 :  0.97 0.9646\n",
            "step 6600 :  0.9709166666666667 0.9642\n",
            "step 7200 :  0.9714166666666667 0.9651\n",
            "step 7800 :  0.9746333333333334 0.9663\n",
            "step 8400 :  0.9767166666666667 0.9686\n",
            "step 9000 :  0.97655 0.9688\n",
            "step 9600 :  0.9788666666666667 0.9695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcRrb2gTW7oC"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    }
  ]
}