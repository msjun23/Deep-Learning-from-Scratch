{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_numerical_differentiation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7VAVPr0dGHLbjbtGsr63S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msjun23/Deep-Learning-from-Scratch/blob/main/Chapter4/2_numerical_differentiation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLRB3a0tCbvR"
      },
      "source": [
        "# 수치 미분"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJimdlUm-3Jm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fac5b694-2d2d-48f1-c0c9-f07da2388550"
      },
      "source": [
        "# 너무 작은 수(소수점 8자리 이하)의 반올리 오차\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "np.float32(1e-50)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzRJskRsCsqr"
      },
      "source": [
        "# 수치 미분 예시\r\n",
        "def numerical_diff(f, x):\r\n",
        "  h = 1e-4    # 0.0001\r\n",
        "  return (f(x + h) - f(x - h)) / (2 * h)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bpP4QQ_DIi3"
      },
      "source": [
        "> - 해석적 미분 : 수식을 전개해 미분. 수학 시간에 배운 것\r\n",
        "> - 수치 미분 : 아주 작은 차분으로 미분하여 근사치를 구하는 것"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69nU9XsdDFzA"
      },
      "source": [
        "# y = 0.01x^2 + 0.1x\r\n",
        "def function_1(x):\r\n",
        "  return 0.01 * x**2 + 0.1 * x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "WI9lIkdWDvMU",
        "outputId": "842d18e4-47a9-4342-c8d6-3255a34c63b9"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "x = np.arange(0.0, 20.0, 0.1)\r\n",
        "y = function_1(x)\r\n",
        "plt.xlabel('x')\r\n",
        "plt.ylabel('f(x)')\r\n",
        "plt.plot(x, y)\r\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa5CgEcNECBU",
        "outputId": "b1100787-b49f-4e0f-9053-cd5fba9e525a"
      },
      "source": [
        "# 수치 미분 예시\r\n",
        "print(numerical_diff(function_1, 5))\r\n",
        "print(numerical_diff(function_1, 10))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.1999999999990898\n",
            "0.2999999999986347\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPDPGaLDEVXR"
      },
      "source": [
        "---\r\n",
        "\r\n",
        "# 편미분"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rMG8ei5EONA"
      },
      "source": [
        "# f(x_0, x_1) = x_0^2 + x_1^2\r\n",
        "def function_2(x):\r\n",
        "  return x[0]**2 + x[1]**2\r\n",
        "  # return np.sum(x**2)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuhD5R9cEtbF",
        "outputId": "6f00dd6c-b83e-4125-a0a1-50cfa4c8f4a3"
      },
      "source": [
        "# x_0 = 3, x_1 = 4 일 때, x_0에 대한 편미분\r\n",
        "def function_tmp1(x0):\r\n",
        "  return x0 * x0 + 4.0**2.0\r\n",
        "\r\n",
        "print(numerical_diff(function_tmp1, 3.0))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.00000000000378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDDNNDghF8-U",
        "outputId": "044a22cd-bd69-42fe-f240-bb33998fd721"
      },
      "source": [
        "# x_0 = 3, x_1 = 4 일 때, x_1에 대한 편미분\r\n",
        "def function_tmp2(x1):\r\n",
        "  return 3.0**2.0 + x1 * x1\r\n",
        "\r\n",
        "print(numerical_diff(function_tmp2, 4.0))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.999999999999119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0aypWrlIta0"
      },
      "source": [
        "# 기울기(Gradient)\r\n",
        "\r\n",
        "모든 편미분을 **벡터**로 정리한 것을 기울기(gradient)라고 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GEdXRiVGeQI"
      },
      "source": [
        "def numerical_gradient(f, x):\r\n",
        "  h = 1e-4    # 0.0001\r\n",
        "  grad = np.zeros_like(x)   # x와 형상이 같은 배열을 생성\r\n",
        "\r\n",
        "  for idx in range(x.size):\r\n",
        "    tmp_val = x[idx]\r\n",
        "\r\n",
        "    # f(x + h) 계산\r\n",
        "    x[idx] = tmp_val + h\r\n",
        "    fxh1 = f(x)\r\n",
        "\r\n",
        "    # f(x - h) 계산\r\n",
        "    x[idx] = tmp_val - h\r\n",
        "    fxh2 = f(x)\r\n",
        "\r\n",
        "    grad[idx] = (fxh1 - fxh2) / (2 * h)\r\n",
        "    x[idx] = tmp_val    # 값 복원\r\n",
        "\r\n",
        "  return grad"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-0xGEzAKHFy",
        "outputId": "22434669-0f7c-4205-dcdc-65982cc5d4b7"
      },
      "source": [
        "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\r\n",
        "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\r\n",
        "print(numerical_gradient(function_2, np.array([3.0, 0.0])))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6. 8.]\n",
            "[0. 4.]\n",
            "[6. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SLrnkhZY9iP"
      },
      "source": [
        "기울기는 방향을 가진 벡터(화살표)로 그려진다. 이때 기울기의 결과에 마이너스를 붙인 벡터를 그려보면, 기울기가 가리키는 방향은 각 장소에서 **함수의 출력 값을 가장 크게 줄이는 방향**이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSwn12PMfQXv"
      },
      "source": [
        "---\r\n",
        "\r\n",
        "# 경사법(경사 하강법)\r\n",
        "\r\n",
        "기울기를 이용해 함수의 최솟값을 찾는것이 경사법, **경사 하강법**이다. 경사법을 수식으로 나타내면 아래와 같다.\r\n",
        "$$x_0=x_0-\\eta\\frac{\\partial{f}}{\\partial{x_0}}\r\n",
        "\\\\x_1=x_1-\\eta\\frac{\\partial{f}}{\\partial{x_1}}$$\r\n",
        "\r\n",
        "여기서 $\\eta$(eta, 에타)는 갱신하는 양을 나타낸다. 이를 신경망에서는 **학습률(learning rate)**라고 한다. 학습률은 한 번의 학습으로 얼마나 학습을 진행하나, 즉 매개변수 값을 얼마나 갱신하느냐를 정하는 것이다.\r\n",
        "\r\n",
        "여기서 learning rate는 미리 특정 값으로 정해두는 **하이퍼파라미터(hyperparameter)**이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "671ByepFhHoO"
      },
      "source": [
        "# 경사 하강법 구현\r\n",
        "def gradient_descent(f, init_x, lr=0.01, step_num=100):\r\n",
        "  x = init_x\r\n",
        "\r\n",
        "  for i in range(step_num):\r\n",
        "    grad = numerical_gradient(f, x)\r\n",
        "    x -= lr * grad\r\n",
        "    \r\n",
        "  return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzKQ9zTHhdGp",
        "outputId": "92957fb2-3bc8-4683-a277-041483036ed8"
      },
      "source": [
        "# 경사 하강법으로 f(x_0, x_1) = x_0^2 + x_1^2 의 최솟값 구하기\r\n",
        "def function_2(x):\r\n",
        "  return x[0]**2 + x[1]**2\r\n",
        "\r\n",
        "init_x = np.array([-3.0, 4.0])\r\n",
        "print(gradient_descent(function_2,init_x=init_x, lr=0.1, step_num=100))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-6.11110793e-10  8.14814391e-10]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUaS822Lkj3G"
      },
      "source": [
        "최종 결과는 거의 (0, 0)에 가까운 결과이다.\r\n",
        "\r\n",
        "학습률은 너무 크거나 작지 않게 잘 조절하는 것이 중요하다. 학습률이 너무 크면 큰 값으로 발산해버리고, 반대로 너무 작으면 거의 갱신되지 않은 채 끝나버린다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keh0HN1OkL9u",
        "outputId": "ff59ab0b-d839-4d1b-b6f3-5dca71a9d598"
      },
      "source": [
        "# 학습률이 너무 큰 예 : lr = 10.0\r\n",
        "init_x = np.array([-3.0, 4.0])\r\n",
        "print(gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-2.58983747e+13 -1.29524862e+12]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vS-fXi50k86k",
        "outputId": "02aeebc4-8b37-46e4-f409-20c35427a398"
      },
      "source": [
        "# 학습률이 너무 작은 예 : lr = 1e-10\r\n",
        "init_x = np.array([-3.0, 4.0])\r\n",
        "print(gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-2.99999994  3.99999992]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQyn8FDLmFYI"
      },
      "source": [
        "신경망 학습에서도 기울기를 구해야 한다. 여기서의 기울기는 가중치 매개변수에 대한 손실 함수의 기울기이다. 가중치가 $\\mathbf{W}$, 손실 함수가 $L$인 신경망에서 경사(기울기)는 $\\frac{\\partial{L}}{\\partial{\\mathbf{W}}}$로 나타낼 수 있다.\r\n",
        "\r\n",
        "$$\\mathbf{W}=\\begin{pmatrix}\r\n",
        "w_{11} &w_{12}  &w_{13} \\\\ \r\n",
        "w_{21} &w_{22}  &w_{23} \r\n",
        "\\end{pmatrix}\r\n",
        "\\\\\\frac{\\partial{L}}{\\partial{\\mathbf{W}}}=\\begin{pmatrix}\r\n",
        "\\frac{\\partial{L}}{\\partial{\\mathbf{W_{11}}}} &\\frac{\\partial{L}}{\\partial{\\mathbf{W_{12}}}}  &\\frac{\\partial{L}}{\\partial{\\mathbf{W_{13}}}} \\\\ \r\n",
        "\\frac{\\partial{L}}{\\partial{\\mathbf{W_{21}}}} &\\frac{\\partial{L}}{\\partial{\\mathbf{W_{22}}}}  &\\frac{\\partial{L}}{\\partial{\\mathbf{W_{23}}}} \r\n",
        "\\end{pmatrix}$$\r\n",
        "\r\n",
        "여기서 각 원소는 각각의 원소에 관한 편미분이다. 예를 들어 1행 1열의 $\\frac{\\partial{L}}{\\partial{\\mathbf{W_{11}}}}$은 $w_{11}$을 조금 변경했을 때 손실 함수 $L$이 얼마나 변화하느냐를 나타냅니다. 이때 중요한 것은 $\\mathbf{W}$와 $\\frac{\\partial{L}}{\\partial{\\mathbf{W}}}$의 형상이 모두 같다는 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoDz63jFlNvU"
      },
      "source": [
        "# simpleNet 클래스 구현\r\n",
        "import sys, os\r\n",
        "sys.path.append(os.pardir)\r\n",
        "from functions import softmax, cross_entropy_error\r\n",
        "from gradient import numerical_gradient\r\n",
        "\r\n",
        "class simpleNet:\r\n",
        "  def __init__(self):\r\n",
        "    self.W = np.random.randn(2, 3)\r\n",
        "\r\n",
        "  def predict(self, x):\r\n",
        "    return np.dot(x, self.W)\r\n",
        "\r\n",
        "  def loss(self, x, t):\r\n",
        "    z = self.predict(x)\r\n",
        "    y = softmax(z)\r\n",
        "    loss = cross_entropy_error(y, t)\r\n",
        "\r\n",
        "    return loss"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiSTCizTpVw1",
        "outputId": "21ac3f50-3389-4d27-edaa-2d325e365c49"
      },
      "source": [
        "net = simpleNet()\r\n",
        "print(net.W)    # 가중치 매개변수\r\n",
        "\r\n",
        "x = np.array([0.6, 0.9])\r\n",
        "p = net.predict(x)\r\n",
        "print(p)\r\n",
        "\r\n",
        "print(np.argmax(p))    # 최댓값의 인덱스\r\n",
        "\r\n",
        "t = np.array([0, 0, 1])   # 정답 레이블\r\n",
        "net.loss(x, t)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.12071411 -0.63606457  0.07759607]\n",
            " [ 1.39299078  0.80447731 -0.50590284]]\n",
            "[ 0.58126323  0.34239084 -0.40875492]\n",
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.7597022354288034"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AKeb4YxpbOi",
        "outputId": "94d402a2-15cf-4ce3-a5ce-fd07b0151890"
      },
      "source": [
        "def f(W):\r\n",
        "  return net.loss(x, t)\r\n",
        "\r\n",
        "dW = numerical_gradient(f, net.W)\r\n",
        "print(dW)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.27789529  0.21884682 -0.49674211]\n",
            " [ 0.41684294  0.32827022 -0.74511317]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcVyMDKpqv_Z",
        "outputId": "dc47e4b4-f131-4af6-c6f9-3797f0c8991c"
      },
      "source": [
        "f = lambda w: net.loss(x, t)\r\n",
        "\r\n",
        "dW = numerical_gradient(f, net.W)\r\n",
        "print(dW)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.27789529  0.21884682 -0.49674211]\n",
            " [ 0.41684294  0.32827022 -0.74511317]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZEgBx3OrgJT"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}